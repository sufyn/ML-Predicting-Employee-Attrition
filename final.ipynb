{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"IBM_HR_Employee_Attrition.csv\")  # Replace with your filename\n",
    "\n",
    "# Explore data\n",
    "print(data.head())  # View first few rows\n",
    "print(data.info())  # View data types and missing values\n",
    "# print(data.corr())\n",
    "# Analyze target variable\n",
    "attrition_count = data[\"Attrition\"].value_counts()\n",
    "print(attrition_count)  # Count of employees leaving vs staying\n",
    "\n",
    "# Explore categorical features\n",
    "categorical_features = data.select_dtypes(include=[\"object\"])\n",
    "for col in categorical_features.columns:\n",
    "  sns.countplot(x=col, data=data)  # Plot count for each category\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Explore numerical features\n",
    "# numerical_features = data.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "# for col in numerical_features.columns:\n",
    "#   sns.histplot(data[col], kde=True)  # Plot histogram with kernel density estimate\n",
    "#   plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "# correlation_matrix = data.corr()\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Download dataset (replace with your download path)\n",
    "# data_path = \"IBM_HR_Employee_Attrition.csv\"\n",
    "\n",
    "# Read data\n",
    "# df = pd.read_csv(data_path)\n",
    "df = data\n",
    "# Define features and target variable\n",
    "features = [col for col in df.columns if col != \"Attrition\"]\n",
    "target = \"Attrition\"\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "# Handle missing values (replace with your chosen imputation strategy)\n",
    "# Example: fill numerical missing values with mean and categorical with mode\n",
    "df[\"DistanceFromHome\"].fillna(df[\"DistanceFromHome\"].mean(), inplace=True)\n",
    "for col in df.select_dtypes(include=['object']):\n",
    "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=['object']):\n",
    "  df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Scale numerical features (if necessary)\n",
    "scaler = StandardScaler()\n",
    "# df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel  # For feature selection\n",
    "from imblearn.over_sampling import SMOTE  # Import for oversampling\n",
    "\n",
    "# Encode categorical features\n",
    "# Define numerical_features\n",
    "numerical_features = [col for col in df.columns if col not in df.select_dtypes(include=['object'])]\n",
    "\n",
    "# Scale numerical features (if necessary)\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# Oversample the minority class (optional)\n",
    "if df[\"Attrition\"].value_counts().min() < df[\"Attrition\"].value_counts().max():\n",
    "  oversample = SMOTE(random_state=42)\n",
    "  X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "  print(f\"Class Distribution (After Oversampling):\")\n",
    "  print(pd.Series(y_train).value_counts())\n",
    "\n",
    "# Feature Selection (using Random Forest as feature importance estimator)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_estimator = RandomForestClassifier(random_state=42)\n",
    "rf_estimator.fit(X_train, y_train)\n",
    "\n",
    "# Select features based on importance scores\n",
    "selector = SelectFromModel(rf_estimator, threshold=0.05)  # Select features with importance > 0.05\n",
    "selector.fit(X_train, y_train)\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "\n",
    "# Reduced feature set for model training\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "\n",
    "# Model Development and Evaluation\n",
    "\n",
    "# Define models (consider adding more models if desired)\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for model_name, model in models.items():\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  print(f\"** Model: {model_name} **\")\n",
    "  print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "  print(f\"Precision: {precision_score(y_test, y_pred,zero_division='warn')}\")\n",
    "  print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "  print(f\"F1-Score: {f1_score(y_test, y_pred)}\")\n",
    "  print(\"-\" * 30)\n",
    "\n",
    "# Model Optimization (using GridSearchCV for Random Forest as an example)\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1_micro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"** Optimized Random Forest Model **\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1-Score: {grid_search.best_score_}\")\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"** Best Model Evaluation **\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred,zero_division='warn')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Classification Report\n",
    "print(\"** Classification Report **\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"** Confusion Matrix **\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "# Extract feature importance from the best model\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "print(\"** Feature Importance **\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_model, \"best_model.pkl\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
